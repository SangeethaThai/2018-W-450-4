{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/2018-W-450-4/03-learning-curves\n"
     ]
    }
   ],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run src/load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2fbde8e4a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from .validation import (as_float_array,\n\u001b[0m\u001b[1;32m     12\u001b[0m                          \u001b[0massert_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMemorizedResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrintTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m CacheItemInfo = collections.namedtuple('CacheItemInfo',\n\u001b[0;32m---> 45\u001b[0;31m                                        'path size last_access')\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# TODO: The following object should have a data store object as a sub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/collections/__init__.py\u001b[0m in \u001b[0;36mnamedtuple\u001b[0;34m(typename, field_names, verbose, rename, module)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;31m# tracing utilities by setting a value for frame.f_globals['__name__']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mnamespace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'namedtuple_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_definition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_source\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install tqdm --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train_df = data['adult']['train']['engineered']\n",
    "adult_train_target = data['adult']['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_training_set(X_train, y_train, n_pcnt):\n",
    "    n = X_train.shape[0]*n_pcnt//100\n",
    "    return n, X_train[:n], y_train[:n]\n",
    "\n",
    "def time_function_call(function_call):\n",
    "    start = time()\n",
    "    result = function_call\n",
    "    execution_time = time() - start\n",
    "    return result, execution_time\n",
    "\n",
    "def run_model(model, model_name, n_pcnt, data, labels):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify=labels)\n",
    "\n",
    "    \n",
    "    n, X_samp, y_samp = sample_training_set(X_train, y_train, n_pcnt)\n",
    "    \n",
    "    _, fit_time = time_function_call(\n",
    "        model.fit(X_samp, y_samp))\n",
    "    \n",
    "    train_pred, train_pred_time = time_function_call(\n",
    "        model.predict(X_samp))\n",
    "    \n",
    "    test_pred, test_pred_time = time_function_call(\n",
    "        model.predict(X_test))    \n",
    "    \n",
    "    return {\n",
    "            'model' : model,\n",
    "            'model_name' : model_name,\n",
    "            'n_pcnt' : n_pcnt,\n",
    "            'n' : n,\n",
    "            'f1_train_score' : f1_score(y_samp, train_pred),\n",
    "            'f1_test_score' : f1_score(y_test, test_pred),\n",
    "            'accuracy_train_score' : model.score(X_samp, y_samp),\n",
    "            'accuracy_test_score' : model.score(X_test, y_test),\n",
    "            'fit_time' : fit_time,\n",
    "            'train_pred_time' : train_pred_time,\n",
    "            'test_pred_time' : test_pred_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Ranking - by Single Feature F$_1$ Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vr_by_f1_test_scores = []\n",
    "for feature in tqdm(adult_train_df.columns):\n",
    "    results = run_model(LogisticRegression(), 'variable ranking', 50, adult_train_df[[feature]], adult_train_target)\n",
    "    test_score = results['f1_test_score']\n",
    "    if test_score > 0:\n",
    "        vr_by_f1_test_scores.append({'feature': feature, 'score' : test_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_f1_results = pd.DataFrame(vr_by_f1_test_scores).sort_values('score', ascending=False)\n",
    "vr_by_f1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_f1_performant_features = list(vr_by_f1_results.feature.values)\n",
    "vr_by_f1_performant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model\n",
    "\n",
    "Add one feature at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_f1_features_to_test = []\n",
    "vr_by_f1_features_test_results = {}\n",
    "for feature in tqdm(vr_by_f1_performant_features):\n",
    "    vr_by_f1_features_to_test.append(feature)\n",
    "    vr_by_f1_features_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n",
    "                                                        adult_train_df[vr_by_f1_features_to_test],\n",
    "                                                        adult_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_f1_features_test_results = pd.DataFrame(vr_by_f1_features_test_results).T\n",
    "vr_by_f1_features_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(vr_by_f1_features_to_test)), vr_by_f1_features_test_results.f1_test_score, label='test performance')\n",
    "plt.plot(range(len(vr_by_f1_features_to_test)), vr_by_f1_features_test_results.f1_train_score, label='train performance')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable-Ranking - By Regression Coefficient in Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model_results = run_model(LogisticRegression(), 'logit', 100,\n",
    "                                 adult_train_df,\n",
    "                                 adult_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_logistic_regression_model = simple_model_results['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = simple_logistic_regression_model.coef_\n",
    "features = adult_train_df.columns\n",
    "coefficients = pd.Series(coefficients.T.ravel(), index=features)\n",
    "coefficients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_coefs = np.abs(coefficients).sort_values(ascending=False)\n",
    "sorted_coefs.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_coef_performant_features = list(sorted_coefs.head(20).index)\n",
    "vr_by_coef_performant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_coef_features_to_test = []\n",
    "vr_by_coef_test_results = {}\n",
    "for feature in tqdm(vr_by_coef_performant_features):\n",
    "    vr_by_coef_features_to_test.append(feature)\n",
    "    vr_by_coef_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n",
    "                                                 adult_train_df[vr_by_coef_features_to_test],\n",
    "                                                 adult_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_coef_test_results = pd.DataFrame(vr_by_coef_test_results).T\n",
    "\n",
    "plt.plot(range(len(vr_by_coef_features_to_test)), vr_by_coef_test_results.f1_test_score, label='test performance')\n",
    "plt.plot(range(len(vr_by_coef_features_to_test)), vr_by_coef_test_results.f1_train_score, label='train performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_coef_with_num_features_to_test = ['age', 'capital-gain','capital-loss','hours-per-week']\n",
    "vr_by_coef_with_num_test_results = {}\n",
    "for feature in tqdm(vr_by_coef_performant_features):\n",
    "    vr_by_coef_with_num_features_to_test.append(feature)\n",
    "    vr_by_coef_with_num_test_results[feature] = run_model(LogisticRegression(), 'logit', 100,\n",
    "                                                          adult_train_df[vr_by_coef_with_num_features_to_test],\n",
    "                                                          adult_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_by_coef_with_num_test_results = pd.DataFrame(vr_by_coef_with_num_test_results).T\n",
    "\n",
    "plt.plot(range(len(vr_by_coef_with_num_features_to_test)-4), vr_by_coef_with_num_test_results.f1_test_score, label='test performance')\n",
    "plt.plot(range(len(vr_by_coef_with_num_features_to_test)-4), vr_by_coef_with_num_test_results.f1_train_score, label='train performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable-Ranking - By Information Gain in Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dtree_results = run_model(DecisionTreeClassifier(), 'dtree', 100,\n",
    "                                 adult_train_df,\n",
    "                                 adult_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dtree_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_decision_tree_model = simple_dtree_results['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = simple_decision_tree_model.feature_importances_\n",
    "features = adult_train_df.columns\n",
    "feature_importances = pd.Series(feature_importances.T.ravel(), index=features)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importances = feature_importances.sort_values(ascending=False)\n",
    "sorted_coefs = np.abs(coefficients).sort_values(ascending=False)\n",
    "sorted_importances = pd.DataFrame(sorted_importances, columns=['importances'])\n",
    "sorted_importances['importance_feature'] = sorted_importances.index\n",
    "sorted_importances.reset_index(drop=True, inplace=True)\n",
    "sorted_coefs = pd.DataFrame(sorted_coefs, columns=['regression coefs'])\n",
    "sorted_coefs['reg_coef_feature'] = sorted_coefs.index\n",
    "sorted_coefs.reset_index(drop=True, inplace=True)\n",
    "sorted_feats = pd.merge(sorted_importances, sorted_coefs, left_index=True, right_index=True)\n",
    "sorted_feats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_performant_features = list(sorted_feats.importance_feature.head(20))\n",
    "importance_performant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_importance_features_to_test = []\n",
    "vr_importance_test_results = {}\n",
    "for feature in tqdm(importance_performant_features):\n",
    "    vr_importance_features_to_test.append(feature)\n",
    "    vr_importance_test_results[feature+'_dtree'] = run_model(DecisionTreeClassifier(), 'dtree', 50,\n",
    "                                                             adult_train_df[vr_importance_features_to_test],\n",
    "                                                             adult_train_target)\n",
    "    vr_importance_test_results[feature+'_logit'] = run_model(LogisticRegression(), 'logit', 50,\n",
    "                                                             adult_train_df[vr_importance_features_to_test],\n",
    "                                                             adult_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_importance_test_results = pd.DataFrame(vr_importance_test_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_results = vr_importance_test_results[vr_importance_test_results.model_name == 'dtree']\n",
    "logit_results = vr_importance_test_results[vr_importance_test_results.model_name == 'logit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,6))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(range(len(vr_importance_features_to_test)), dtree_results.f1_test_score, label='dtree test performance')\n",
    "plt.plot(range(len(vr_importance_features_to_test)), dtree_results.f1_train_score, label='dtree train performance')\n",
    "plt.legend()\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.plot(range(len(vr_importance_features_to_test)), logit_results.f1_test_score, label='logit test performance')\n",
    "plt.plot(range(len(vr_importance_features_to_test)), logit_results.f1_train_score, label='logit train performance')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whos list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20,6))\n",
    "fig.add_subplot(1,5,1)\n",
    "plt.plot(range(len(vr_by_f1_features_to_test)), vr_by_f1_features_test_results.f1_test_score, label='test performance')\n",
    "plt.plot(range(len(vr_by_f1_features_to_test)), vr_by_f1_features_test_results.f1_train_score, label='train performance')\n",
    "plt.title('Adding Variable by F1 score\\n as single variable classifier')\n",
    "plt.legend()\n",
    "fig.add_subplot(1,5,2)\n",
    "plt.plot(range(len(vr_by_coef_features_to_test)), vr_by_coef_test_results.f1_test_score, label='test performance')\n",
    "plt.plot(range(len(vr_by_coef_features_to_test)), vr_by_coef_test_results.f1_train_score, label='train performance')\n",
    "plt.title('Adding Variable by reg coef\\n on full model')\n",
    "plt.legend()\n",
    "fig.add_subplot(1,5,3)\n",
    "plt.plot(range(len(vr_by_coef_with_num_features_to_test)-4), vr_by_coef_with_num_test_results.f1_test_score, label='test performance')\n",
    "plt.plot(range(len(vr_by_coef_with_num_features_to_test)-4), vr_by_coef_with_num_test_results.f1_train_score, label='train performance')\n",
    "plt.title('Adding Variable by reg coef\\n on full model\\n(including numerical)')\n",
    "plt.legend()\n",
    "fig.add_subplot(1,5,4)\n",
    "plt.plot(range(len(vr_importance_features_to_test)), dtree_results.f1_test_score, label='dtree test performance')\n",
    "plt.plot(range(len(vr_importance_features_to_test)), dtree_results.f1_train_score, label='dtree train performance')\n",
    "plt.title('Adding Variable by information gain\\n on decision tree')\n",
    "plt.legend()\n",
    "fig.add_subplot(1,5,5)\n",
    "plt.plot(range(len(vr_importance_features_to_test)), logit_results.f1_test_score, label='logit test performance')\n",
    "plt.plot(range(len(vr_importance_features_to_test)), logit_results.f1_train_score, label='logit train performance')\n",
    "plt.title('Adding Variable by information gain\\n on logistic regression')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(len(vr_importance_features_to_test)), dtree_results.f1_test_score, label='dtree test performance')\n",
    "plt.plot(range(len(vr_importance_features_to_test)), dtree_results.f1_train_score, label='dtree train performance')\n",
    "plt.title('Adding Variable by information gain\\n on decision tree')\n",
    "plt.xticks(range(len(vr_importance_features_to_test)), vr_importance_features_to_test, rotation='vertical')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(len(vr_importance_features_to_test)), logit_results.f1_test_score, label='dtree test performance')\n",
    "plt.plot(range(len(vr_importance_features_to_test)), logit_results.f1_train_score, label='dtree train performance')\n",
    "plt.title('Adding Variable by information gain\\n on logistic regression')\n",
    "plt.xticks(range(len(vr_importance_features_to_test)), vr_importance_features_to_test, rotation='vertical')\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
